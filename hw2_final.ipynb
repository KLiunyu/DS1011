{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/Academic/Nyu/DS1011'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.DataFrame.from_csv(\"/mnt/d/Academic/Nyu/DS1011/hw2_data/snli_train.tsv\", sep='\\t', index_col = None)\n",
    "val_data = pd.DataFrame.from_csv(\"/mnt/d/Academic/Nyu/DS1011/hw2_data/snli_val.tsv\", sep='\\t', index_col = None)\n",
    "\n",
    "def convert_to_chars(data):\n",
    "    return [[t for t in sample.split()] for sample in data]\n",
    "\n",
    "train_data_1 = convert_to_chars(train_data['sentence1'])\n",
    "train_data_2 = convert_to_chars(train_data['sentence2'])\n",
    "\n",
    "val_data_1 = convert_to_chars(val_data['sentence1'])\n",
    "val_data_2 = convert_to_chars(val_data['sentence2'])\n",
    "\n",
    "train_label = train_data['label']\n",
    "val_label = val_data['label']\n",
    "\n",
    "max_len_1 = max([len(s) for s in train_data_1])\n",
    "max_len_2 = max([len(s) for s in train_data_2])\n",
    "\n",
    "val_max_len_1 = max([len(s) for s in val_data_1])\n",
    "val_max_len_2 = max([len(s) for s in val_data_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dic = {}\n",
    "label_dic['contradiction'] = 2\n",
    "label_dic['entailment'] = 1\n",
    "label_dic['neutral'] = 0\n",
    "train_label_vector = [label_dic[i] for i in train_label]\n",
    "val_label_vector = [label_dic[i] for i in val_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-4-a0e0e0ec007d>(23)<module>()\n",
      "-> for i, line in enumerate(f):\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "ft_home = '/mnt/d/Academic/Nyu/DS1011/'\n",
    "words_to_load = 100000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load, 300))\n",
    "    token2id_ft = {}\n",
    "    id2token_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    \n",
    "    token2id_ft['<pad>'] = 0\n",
    "    id2token_ft[0] = '<pad>'\n",
    "    token2id_ft['<unk>'] = 1\n",
    "    id2token_ft[1] = '<unk>' \n",
    "    tmp1 = np.zeros((1,300))\n",
    "    tmp2 = np.random.rand(1,300)\n",
    "    tmp = np.append(tmp1, tmp2, axis = 0)\n",
    "    loaded_embeddings_ft = np.append(tmp, loaded_embeddings_ft, axis = 0)\n",
    "    \n",
    "    import pdb; pdb.set_trace()\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        #print(i)\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        token2id_ft[s[0]] = i + 2\n",
    "        id2token_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 95132 ; token pro-gay\n",
      "Token pro-gay; token id 95132\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token_ft)-1)\n",
    "random_token = id2token_ft[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token_ft[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id_ft[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2token_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, st1, st2, max_len_1, max_len_2, label, token2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.sentence1 = st1\n",
    "        self.sentence2 = st2\n",
    "        self.max_len_1 = max_len_1\n",
    "        self.max_len_2 = max_len_2\n",
    "        self.target_list = label\n",
    "        assert (len(self.sentence1) == len(self.target_list))\n",
    "        assert (len(self.sentence2) == len(self.target_list))\n",
    "        self.token2id = token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx_1 = [self.token2id[c] if c in self.token2id.keys() else self.token2id['<unk>']  for c in self.sentence1[key][:self.max_len_1]]\n",
    "        token_idx_2 = [self.token2id[c] if c in self.token2id.keys() else self.token2id['<unk>']  for c in self.sentence2[key][:self.max_len_2]]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx_1, len(token_idx_1), token_idx_2, len(token_idx_2), label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_1 = []\n",
    "    data_list_2 = []\n",
    "    label_list = []\n",
    "    length_list_1 = []\n",
    "    length_list_2 = []\n",
    "    label_map = {}\n",
    "\n",
    "    \n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list_1.append(datum[1])\n",
    "        length_list_2.append(datum[3])\n",
    "    # padding\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0, max_len_1 - datum[1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_2 = np.pad(np.array(datum[2]),\n",
    "                                pad_width=((0, max_len_2 - datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_1.append(padded_vec_1)\n",
    "        data_list_2.append(padded_vec_2)\n",
    "    ind_dec_order_1 = np.argsort(length_list_1)[::-1]\n",
    "    ind_dec_order_2 = np.argsort(length_list_2)[::-1]\n",
    "    \n",
    "    reverse_order_1 = []\n",
    "    reverse_order_2 = []\n",
    "    #l1_cpy = length_list_1.copy()\n",
    "    \n",
    "    for i in range(len(ind_dec_order_1)):\n",
    "        reverse_order_1.append(ind_dec_order_1.tolist().index(i))\n",
    "    for i in range(len(ind_dec_order_2)):\n",
    "        reverse_order_2.append(ind_dec_order_2.tolist().index(i))\n",
    "\n",
    "    data_list_1 = np.array(data_list_1)[ind_dec_order_1]\n",
    "    length_list_1 = np.array(length_list_1)[ind_dec_order_1]\n",
    "    data_list_2 = np.array(data_list_2)[ind_dec_order_2]\n",
    "    length_list_2 = np.array(length_list_2)[ind_dec_order_2]\n",
    "    #import pdb; pdb.set_trace()\n",
    "    return [torch.from_numpy(np.array(data_list_1)), torch.LongTensor(length_list_1), torch.from_numpy(np.array(data_list_2)), torch.LongTensor(length_list_2), torch.LongTensor(label_list), reverse_order_1, reverse_order_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "\n",
    "train_dataset = VocabDataset(train_data_1, train_data_2, max_len_1, max_len_2, train_label_vector, token2id_ft)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_data_1, val_data_2, val_max_len_1, val_max_len_2, val_label_vector, token2id_ft)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "#train_dataset_2 = VocabDataset(train_data_2, token2id_ft)\n",
    "#train_loader_2 = torch.utils.data.DataLoader(dataset=train_dataset_2,\n",
    "#                                           batch_size=BATCH_SIZE,\n",
    "#                                           collate_fn=vocab_collate_func,\n",
    "#                   shuffle=True)\n",
    "\n",
    "#val_dataset_1 = VocabDataset(val_data_1, token2id_ft)\n",
    "#val_loader_1 = torch.utils.data.DataLoader(dataset=val_dataset_1,\n",
    "#                                           batch_size=BATCH_SIZE,\n",
    "#                                           collate_fn=vocab_collate_func,\n",
    "#                                           shuffle=True)\n",
    "\n",
    "#val_dataset_2 = VocabDataset(val_data_2, token2id_ft)\n",
    "#val_loader_2 = torch.utils.data.DataLoader(dataset=val_dataset_2,\n",
    "#                                           batch_size=BATCH_SIZE,\n",
    "#                                           collate_fn=vocab_collate_func,\n",
    "#                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size, pt_embed):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        PAD_IDX = 0\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #self.embedding.weight.data.copy_(pt_embed)\n",
    "        self.embedding.from_pretrained(pt_embed, freeze = True)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional =  True)\n",
    "        \n",
    "        self.tf = nn.Sequential(nn.Linear(2*hidden_size, hidden_size), nn.ReLU(inplace=True),\n",
    "                                nn.Linear(hidden_size,num_classes))\n",
    "        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x_1, lengths_1, x_2, lengths_2, rev_od_1, rev_od_2):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size, seq_len = x_1.size()\n",
    "        \n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        #self.hidden_1 = self.init_hidden(batch_size)\n",
    "        #self.hidden_2 = self.init_hidden(batch_size)\n",
    "        \n",
    "        encoded = torch.zeros(0,0)\n",
    "\n",
    "        # get embedding of characters\n",
    "        embed_1 = self.embedding(x_1)\n",
    "        embed_2 = self.embedding(x_2)\n",
    "        \n",
    "        # pack padded sequence\n",
    "        embed_1 = torch.nn.utils.rnn.pack_padded_sequence(embed_1, lengths_1.numpy(), batch_first=True)\n",
    "        embed_2 = torch.nn.utils.rnn.pack_padded_sequence(embed_2, lengths_2.numpy(), batch_first=True)\n",
    "        # fprop though RNN\n",
    "        rnn_out, self.hidden_1 = self.rnn(embed_1)\n",
    "        rnn_out, self.hidden_2 = self.rnn(embed_2)\n",
    "        \n",
    "        #import pdb; pdb.set_trace()\n",
    "        encoded = torch.cat([torch.index_select(self.hidden_1.permute(1,0,2).sum(dim = 1),0,torch.LongTensor(rev_od_1)), torch.index_select(self.hidden_2.permute(1,0,2).sum(dim = 1),0,torch.LongTensor(rev_od_2))], dim = 1)\n",
    "        #encoded = torch.mul(torch.index_select(self.hidden_1.permute(1,0,2).sum(dim = 1),0,torch.LongTensor(rev_od_1)), torch.index_select(self.hidden_2.permute(1,0,2).sum(dim = 1),0,torch.LongTensor(rev_od_2)))\n",
    "        \n",
    "        logits = self.tf(encoded)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clliu/anaconda3/lib/python3.5/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/3125], Validation Acc: 41.6\n",
      "tensor(1.0742, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [201/3125], Validation Acc: 44.6\n",
      "tensor(1.0673, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [301/3125], Validation Acc: 50.9\n",
      "tensor(0.9605, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [401/3125], Validation Acc: 53.1\n",
      "tensor(0.9856, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [501/3125], Validation Acc: 55.0\n",
      "tensor(0.7989, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [601/3125], Validation Acc: 57.8\n",
      "tensor(1.0446, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [701/3125], Validation Acc: 59.7\n",
      "tensor(0.8513, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [801/3125], Validation Acc: 57.4\n",
      "tensor(1.0666, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [901/3125], Validation Acc: 60.2\n",
      "tensor(0.9507, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1001/3125], Validation Acc: 59.0\n",
      "tensor(1.0501, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1101/3125], Validation Acc: 58.5\n",
      "tensor(0.8101, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1201/3125], Validation Acc: 60.3\n",
      "tensor(0.8394, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1301/3125], Validation Acc: 61.5\n",
      "tensor(0.8657, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1401/3125], Validation Acc: 61.5\n",
      "tensor(0.8651, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1501/3125], Validation Acc: 59.7\n",
      "tensor(0.8365, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1601/3125], Validation Acc: 60.8\n",
      "tensor(0.9004, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1701/3125], Validation Acc: 61.6\n",
      "tensor(0.8727, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1801/3125], Validation Acc: 62.1\n",
      "tensor(0.7649, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [1901/3125], Validation Acc: 61.8\n",
      "tensor(0.7606, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2001/3125], Validation Acc: 64.7\n",
      "tensor(0.8198, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2101/3125], Validation Acc: 62.3\n",
      "tensor(0.8851, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2201/3125], Validation Acc: 63.1\n",
      "tensor(0.7768, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2301/3125], Validation Acc: 63.3\n",
      "tensor(0.8333, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2401/3125], Validation Acc: 63.6\n",
      "tensor(0.8135, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2501/3125], Validation Acc: 65.3\n",
      "tensor(0.8220, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2601/3125], Validation Acc: 65.6\n",
      "tensor(0.6550, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2701/3125], Validation Acc: 64.7\n",
      "tensor(0.7755, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2801/3125], Validation Acc: 64.1\n",
      "tensor(0.8727, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [2901/3125], Validation Acc: 64.0\n",
      "tensor(0.7710, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [3001/3125], Validation Acc: 62.7\n",
      "tensor(0.7090, grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [3101/3125], Validation Acc: 64.4\n",
      "tensor(0.7841, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [101/3125], Validation Acc: 64.1\n",
      "tensor(0.7244, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [201/3125], Validation Acc: 63.3\n",
      "tensor(0.5786, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [301/3125], Validation Acc: 63.7\n",
      "tensor(0.6882, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [401/3125], Validation Acc: 63.7\n",
      "tensor(0.6765, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [501/3125], Validation Acc: 65.8\n",
      "tensor(0.5579, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [601/3125], Validation Acc: 63.3\n",
      "tensor(0.7997, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [701/3125], Validation Acc: 64.3\n",
      "tensor(0.5971, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [801/3125], Validation Acc: 64.4\n",
      "tensor(0.8212, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [901/3125], Validation Acc: 66.0\n",
      "tensor(0.4828, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1001/3125], Validation Acc: 64.9\n",
      "tensor(0.8131, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1101/3125], Validation Acc: 62.2\n",
      "tensor(0.7330, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1201/3125], Validation Acc: 65.3\n",
      "tensor(0.8148, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1301/3125], Validation Acc: 66.1\n",
      "tensor(0.5455, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1401/3125], Validation Acc: 65.1\n",
      "tensor(0.9033, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1501/3125], Validation Acc: 63.9\n",
      "tensor(0.7337, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1601/3125], Validation Acc: 64.5\n",
      "tensor(0.6799, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1701/3125], Validation Acc: 65.3\n",
      "tensor(0.7272, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1801/3125], Validation Acc: 65.8\n",
      "tensor(0.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [1901/3125], Validation Acc: 65.2\n",
      "tensor(0.8772, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2001/3125], Validation Acc: 65.5\n",
      "tensor(0.6955, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2101/3125], Validation Acc: 65.8\n",
      "tensor(0.5183, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2201/3125], Validation Acc: 65.5\n",
      "tensor(0.9724, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2301/3125], Validation Acc: 65.0\n",
      "tensor(0.8344, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2401/3125], Validation Acc: 66.1\n",
      "tensor(0.7714, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2501/3125], Validation Acc: 66.5\n",
      "tensor(0.8078, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2601/3125], Validation Acc: 66.0\n",
      "tensor(0.8636, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2701/3125], Validation Acc: 67.1\n",
      "tensor(0.7218, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2801/3125], Validation Acc: 67.2\n",
      "tensor(0.5114, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [2901/3125], Validation Acc: 68.1\n",
      "tensor(0.7003, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [3001/3125], Validation Acc: 67.7\n",
      "tensor(0.8736, grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [3101/3125], Validation Acc: 68.4\n",
      "tensor(0.7727, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [101/3125], Validation Acc: 66.8\n",
      "tensor(0.5373, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [201/3125], Validation Acc: 68.2\n",
      "tensor(0.5175, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [301/3125], Validation Acc: 66.8\n",
      "tensor(0.6961, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [401/3125], Validation Acc: 67.2\n",
      "tensor(0.5821, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [501/3125], Validation Acc: 66.0\n",
      "tensor(0.5978, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [601/3125], Validation Acc: 66.6\n",
      "tensor(0.6118, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [701/3125], Validation Acc: 66.2\n",
      "tensor(0.5457, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [801/3125], Validation Acc: 65.8\n",
      "tensor(0.5694, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [901/3125], Validation Acc: 64.8\n",
      "tensor(0.2394, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1001/3125], Validation Acc: 66.8\n",
      "tensor(0.6936, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1101/3125], Validation Acc: 65.7\n",
      "tensor(0.5659, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1201/3125], Validation Acc: 64.8\n",
      "tensor(0.6690, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1301/3125], Validation Acc: 64.8\n",
      "tensor(0.5866, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1401/3125], Validation Acc: 65.5\n",
      "tensor(0.4791, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1501/3125], Validation Acc: 65.6\n",
      "tensor(0.6894, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1601/3125], Validation Acc: 67.8\n",
      "tensor(0.6691, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1701/3125], Validation Acc: 64.7\n",
      "tensor(0.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1801/3125], Validation Acc: 66.1\n",
      "tensor(0.5698, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [1901/3125], Validation Acc: 66.3\n",
      "tensor(0.6936, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [2001/3125], Validation Acc: 68.0\n",
      "tensor(0.7368, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [2101/3125], Validation Acc: 66.9\n",
      "tensor(0.7329, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [2201/3125], Validation Acc: 67.1\n",
      "tensor(0.5771, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [2301/3125], Validation Acc: 67.9\n",
      "tensor(0.7068, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [2401/3125], Validation Acc: 65.9\n",
      "tensor(0.5348, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/5], Step: [2501/3125], Validation Acc: 67.4\n",
      "tensor(0.8567, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [2601/3125], Validation Acc: 66.7\n",
      "tensor(0.7262, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [2701/3125], Validation Acc: 66.3\n",
      "tensor(0.8830, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [2801/3125], Validation Acc: 66.7\n",
      "tensor(0.6798, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [2901/3125], Validation Acc: 66.8\n",
      "tensor(0.5617, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [3001/3125], Validation Acc: 66.3\n",
      "tensor(0.7087, grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [3101/3125], Validation Acc: 67.4\n",
      "tensor(0.5796, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [101/3125], Validation Acc: 67.0\n",
      "tensor(0.2593, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [201/3125], Validation Acc: 66.5\n",
      "tensor(0.7353, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [301/3125], Validation Acc: 67.5\n",
      "tensor(0.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [401/3125], Validation Acc: 66.6\n",
      "tensor(0.6586, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [501/3125], Validation Acc: 67.1\n",
      "tensor(0.4402, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [601/3125], Validation Acc: 66.2\n",
      "tensor(0.4178, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [701/3125], Validation Acc: 64.4\n",
      "tensor(0.7017, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [801/3125], Validation Acc: 64.7\n",
      "tensor(0.3711, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [901/3125], Validation Acc: 66.1\n",
      "tensor(0.7252, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1001/3125], Validation Acc: 65.6\n",
      "tensor(0.4014, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1101/3125], Validation Acc: 65.0\n",
      "tensor(0.5476, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1201/3125], Validation Acc: 64.8\n",
      "tensor(0.8989, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1301/3125], Validation Acc: 62.9\n",
      "tensor(0.3352, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1401/3125], Validation Acc: 64.4\n",
      "tensor(0.3713, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1501/3125], Validation Acc: 63.7\n",
      "tensor(0.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1601/3125], Validation Acc: 64.5\n",
      "tensor(0.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1701/3125], Validation Acc: 65.1\n",
      "tensor(0.3689, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1801/3125], Validation Acc: 65.5\n",
      "tensor(0.5757, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [1901/3125], Validation Acc: 66.1\n",
      "tensor(0.5960, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2001/3125], Validation Acc: 65.7\n",
      "tensor(0.6423, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2101/3125], Validation Acc: 66.9\n",
      "tensor(0.5035, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2201/3125], Validation Acc: 66.9\n",
      "tensor(0.7791, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2301/3125], Validation Acc: 66.5\n",
      "tensor(0.6901, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2401/3125], Validation Acc: 67.2\n",
      "tensor(0.6823, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2501/3125], Validation Acc: 66.5\n",
      "tensor(0.3856, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2601/3125], Validation Acc: 65.7\n",
      "tensor(0.5937, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2701/3125], Validation Acc: 66.4\n",
      "tensor(0.5529, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2801/3125], Validation Acc: 66.0\n",
      "tensor(0.4609, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [2901/3125], Validation Acc: 66.0\n",
      "tensor(0.4863, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [3001/3125], Validation Acc: 66.3\n",
      "tensor(0.6234, grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [3101/3125], Validation Acc: 65.8\n",
      "tensor(0.5303, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [101/3125], Validation Acc: 65.8\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [201/3125], Validation Acc: 64.4\n",
      "tensor(0.3395, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [301/3125], Validation Acc: 64.2\n",
      "tensor(0.4279, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [401/3125], Validation Acc: 66.3\n",
      "tensor(0.3973, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [501/3125], Validation Acc: 65.2\n",
      "tensor(0.5112, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [601/3125], Validation Acc: 67.5\n",
      "tensor(0.4060, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [701/3125], Validation Acc: 66.1\n",
      "tensor(0.5230, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [801/3125], Validation Acc: 63.7\n",
      "tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [901/3125], Validation Acc: 66.9\n",
      "tensor(0.4437, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1001/3125], Validation Acc: 66.3\n",
      "tensor(0.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1101/3125], Validation Acc: 64.8\n",
      "tensor(0.2913, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1201/3125], Validation Acc: 64.1\n",
      "tensor(0.5560, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1301/3125], Validation Acc: 64.3\n",
      "tensor(0.7534, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1401/3125], Validation Acc: 65.6\n",
      "tensor(0.5221, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1501/3125], Validation Acc: 65.2\n",
      "tensor(0.6039, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1601/3125], Validation Acc: 64.7\n",
      "tensor(0.6409, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1701/3125], Validation Acc: 63.1\n",
      "tensor(0.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1801/3125], Validation Acc: 64.5\n",
      "tensor(0.5674, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [1901/3125], Validation Acc: 65.4\n",
      "tensor(0.6031, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2001/3125], Validation Acc: 66.8\n",
      "tensor(0.7328, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2101/3125], Validation Acc: 65.6\n",
      "tensor(0.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2201/3125], Validation Acc: 64.9\n",
      "tensor(0.4106, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2301/3125], Validation Acc: 64.1\n",
      "tensor(0.5283, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2401/3125], Validation Acc: 67.3\n",
      "tensor(0.3512, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2501/3125], Validation Acc: 66.5\n",
      "tensor(0.7641, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2601/3125], Validation Acc: 66.6\n",
      "tensor(0.3802, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2701/3125], Validation Acc: 66.4\n",
      "tensor(0.4466, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2801/3125], Validation Acc: 66.8\n",
      "tensor(0.6530, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [2901/3125], Validation Acc: 66.4\n",
      "tensor(0.3077, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [3001/3125], Validation Acc: 66.1\n",
      "tensor(0.4517, grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [3101/3125], Validation Acc: 65.0\n",
      "tensor(0.5918, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#from torch \n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_1, lengths_1, data_2, lengths_2, labels, rev_od_1, rev_od_2 in loader:\n",
    "        outputs = F.softmax(model(data_1, lengths_1, data_2, lengths_2, rev_od_1, rev_od_2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = RNN(emb_size=300, hidden_size=200, num_layers=1, num_classes=3, vocab_size=len(id2token_ft), pt_embed = torch.FloatTensor(loaded_embeddings_ft))\n",
    "\n",
    "learning_rate = 0.002\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_1, lengths_1, data_2, lengths_2, labels, rev_od_1, rev_od_2) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        #import pdb; pdb.set_trace()\n",
    "        outputs = model(data_1, lengths_1, data_2, lengths_2, rev_od_1, rev_od_2)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(i)\n",
    "        \n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            if val_acc >= best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model, \"best_rnn_200_mult.pt\")\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size, pt_embed):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.from_pretrained(pt_embed, freeze = True)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.tf = nn.Sequential(nn.Linear(2*hidden_size, hidden_size), nn.ReLU(inplace=True),\n",
    "                                nn.Linear(hidden_size,num_classes))\n",
    "\n",
    "    def forward(self, x_1, lengths_1, x_2, lengths_2, rev_od_1, rev_od_2):\n",
    "        data_1 = torch.index_select(x_1, 0, torch.LongTensor(rev_od_1))\n",
    "        data_2 = torch.index_select(x_2, 0, torch.LongTensor(rev_od_2))\n",
    "        batch_size, seq_len_1 = data_1.size()\n",
    "        batch_size, seq_len_2 = data_2.size()\n",
    "        \n",
    "        #import pdb; pdb.set_trace()\n",
    "        embed_1 = self.embedding(data_1)\n",
    "        hidden_1 = self.conv1(embed_1.transpose(1,2)).transpose(1,2)\n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1, hidden_1.size(-1))).view(batch_size, seq_len_1, hidden_1.size(-1))\n",
    "\n",
    "        hidden_1 = self.conv2(hidden_1.transpose(1,2)).transpose(1,2)\n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1, hidden_1.size(-1))).view(batch_size, seq_len_1, hidden_1.size(-1))\n",
    "        \n",
    "        embed_2 = self.embedding(data_2)\n",
    "        hidden_2 = self.conv3(embed_2.transpose(1,2)).transpose(1,2)\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, seq_len_2, hidden_2.size(-1))\n",
    "\n",
    "        hidden_2 = self.conv4(hidden_2.transpose(1,2)).transpose(1,2)\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, seq_len_2, hidden_2.size(-1))\n",
    "    \n",
    "    \n",
    "        self.maxpool1 = nn.MaxPool1d(seq_len_1)\n",
    "        self.maxpool2 = nn.MaxPool1d(seq_len_2)\n",
    "        \n",
    "        #import pdb; pdb.set_trace()\n",
    "        hidden_1 = self.maxpool1(hidden_1.permute(0,2,1)).sum(dim = 2)\n",
    "        hidden_2 = self.maxpool2(hidden_2.permute(0,2,1)).sum(dim = 2)\n",
    "        encoded = torch.cat([hidden_1, hidden_2], dim = 1)\n",
    "        \n",
    "        logits = self.tf(encoded)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_1, lengths_1, data_2, lengths_2, labels, rev_od_1, rev_od_2 in loader:\n",
    "        outputs = F.softmax(model(data_1, lengths_1, data_2, lengths_2, rev_od_1, rev_od_2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=200, num_layers=1, num_classes=3, vocab_size=len(id2token_ft), pt_embed = torch.FloatTensor(loaded_embeddings_ft))\n",
    "\n",
    "learning_rate = 0.005\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_1, lengths_1, data_2, lengths_2, labels, rev_od_1, rev_od_2) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        #import pdb; pdb.set_trace()\n",
    "        outputs = model(data_1, lengths_1, data_2, lengths_2, rev_od_1, rev_od_2)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(i)\n",
    "        \n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            if val_acc >= best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(the_model, \"best_cnn.pwf\")\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.DataFrame.from_csv(\"/mnt/d/Academic/Nyu/DS1011/hw2_data/mnli_train.tsv\", sep='\\t', index_col = None)\n",
    "val_data = pd.DataFrame.from_csv(\"/mnt/d/Academic/Nyu/DS1011/hw2_data/mnli_val.tsv\", sep='\\t', index_col = None)\n",
    "\n",
    "def convert_to_chars(data):\n",
    "    return [[t for t in sample.split()] for sample in data]\n",
    "\n",
    "genre = train_data['genre'].unique()\n",
    "\n",
    "train_data_1 = {}\n",
    "train_data_2 = {}\n",
    "\n",
    "val_data_1 = {}\n",
    "val_data_2 = {}\n",
    "\n",
    "train_label = {}\n",
    "val_label = {}\n",
    "\n",
    "max_len_1 = {}\n",
    "max_len_2 = {}\n",
    "\n",
    "val_max_len_1 = {}\n",
    "val_max_len_2 = {}\n",
    "\n",
    "label_dic = {}\n",
    "label_dic['contradiction'] = 2\n",
    "label_dic['entailment'] = 1\n",
    "label_dic['neutral'] = 0\n",
    "\n",
    "train_label_vector = {}\n",
    "val_label_vector = {}\n",
    "\n",
    "\n",
    "for i in genre:\n",
    "    train_data_1[i] = convert_to_chars(train_data[train_data['genre'] == i]['sentence1'])\n",
    "    train_data_2[i] = convert_to_chars(train_data[train_data['genre'] == i]['sentence2'])\n",
    "    \n",
    "    val_data_1[i] = convert_to_chars(val_data[val_data['genre'] == i]['sentence1'])\n",
    "    val_data_2[i] = convert_to_chars(val_data[val_data['genre'] == i]['sentence2'])\n",
    "    \n",
    "    train_label[i] = train_data[train_data['genre'] == i]['label']\n",
    "    val_label[i] = val_data[val_data['genre'] == i]['label']\n",
    "    \n",
    "    max_len_1[i] = max([len(s) for s in train_data_1[i]])\n",
    "    max_len_2[i] = max([len(s) for s in train_data_2[i]])\n",
    "    \n",
    "    train_label_vector[i] = [label_dic[x] for x in train_label[i]]\n",
    "    val_label_vector[i] = [label_dic[x] for x in val_label[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test \n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_1, lengths_1, data_2, lengths_2, labels, rev_od_1, rev_od_2 in loader:\n",
    "        outputs = F.softmax(model(data_1, lengths_1, data_2, lengths_2, rev_od_1, rev_od_2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for i in genre:\n",
    "    max_len_1 = max([len(s) for s in val_data_1[i]])\n",
    "    max_len_2 = max([len(s) for s in val_data_2[i]])\n",
    "    \n",
    "    val_dataset = VocabDataset(val_data_1[i], val_data_2[i], val_max_len_1, val_max_len_2, val_label_vector[i], token2id_ft)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "    best_model = torch.load('best_rnn_2.pt')\n",
    "    val_acc = test_model(val_loader, best_model)\n",
    "    print(i)\n",
    "    print(val_acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
